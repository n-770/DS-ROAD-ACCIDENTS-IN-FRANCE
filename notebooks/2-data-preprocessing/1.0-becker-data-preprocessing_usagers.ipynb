{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a4a9d5",
   "metadata": {},
   "source": [
    "Steps to 1.0-becker-data-preprocessing_usagers\n",
    "- from raw datasets to uploaded file.\n",
    "- interpretation and suggestions for next steps.\n",
    "- A Joblib snapshot of the merged dataset already exists in the repository: data/processed/2_preprocessing/1.0-becker-data-preprocessing_usagers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33b6f8",
   "metadata": {},
   "source": [
    "# Libraries Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "021ac897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7697b833",
   "metadata": {},
   "source": [
    "# Raw-data import \n",
    "- usagers files from 2019-2024 from BAAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead9e284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shapes: {2019: (132977, 15), 2020: (105295, 15), 2021: (129248, 16), 2022: (126662, 16), 2023: (125789, 16), 2024: (125187, 16)}\n"
     ]
    }
   ],
   "source": [
    "usagers_2019 = pd.read_csv(\"data/usagers-2019.csv\", sep=\";\")\n",
    "usagers_2020 = pd.read_csv(\"data/usagers-2020.csv\", sep=\";\")\n",
    "usagers_2021 = pd.read_csv(\"data/usagers-2021.csv\", sep=\";\")\n",
    "usagers_2022 = pd.read_csv(\"data/usagers-2022.csv\", sep=\";\")\n",
    "usagers_2023 = pd.read_csv(\"data/usagers-2023.csv\", sep=\";\")\n",
    "usagers_2024 = pd.read_csv(\"data/usagers-2024.csv\", sep=\";\")\n",
    "print(\"Loaded shapes:\", {2019: usagers_2019.shape, 2020: usagers_2020.shape, 2021: usagers_2021.shape,\n",
    "                        2022: usagers_2022.shape, 2023: usagers_2023.shape, 2024: usagers_2024.shape})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c9fa9",
   "metadata": {},
   "source": [
    "# Cleaning \"-1\" Values in \"grav\" and \"an_nais\"\n",
    "- as seen in exploration: -1 in \"grav\" and \"-1\" in \"an_nais\" are unimportant and useless information for project goal\n",
    "- in this step i remove these rows and display before/after/dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6aede44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: rows_before=132977, rows_with_-1_in_grav_or_an_nais=0\n",
      "2020: rows_before=105295, rows_with_-1_in_grav_or_an_nais=0\n",
      "2021: rows_before=129248, rows_with_-1_in_grav_or_an_nais=60\n",
      "2022: rows_before=126662, rows_with_-1_in_grav_or_an_nais=241\n",
      "2023: rows_before=125789, rows_with_-1_in_grav_or_an_nais=118\n",
      "2024: rows_before=125187, rows_with_-1_in_grav_or_an_nais=0\n"
     ]
    }
   ],
   "source": [
    "# Precheck\n",
    "for name, df in [(\"2019\", usagers_2019), (\"2020\", usagers_2020), (\"2021\", usagers_2021),\n",
    "                 (\"2022\", usagers_2022), (\"2023\", usagers_2023), (\"2024\", usagers_2024)]:\n",
    "    rows_before = len(df)\n",
    "    count_minus1 = 0\n",
    "    if \"grav\" in df.columns:\n",
    "        count_minus1 += (df[\"grav\"] == -1).sum()\n",
    "    if \"an_nais\" in df.columns:\n",
    "        count_minus1 += (df[\"an_nais\"] == -1).sum()\n",
    "    print(f\"{name}: rows_before={rows_before}, rows_with_-1_in_grav_or_an_nais={int(count_minus1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3353e69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: before = 132977 after = 132977 dropped = 0\n",
      "2020: before = 105295 after = 105295 dropped = 0\n",
      "2021: before = 129248 after = 129188 dropped = 60\n",
      "2022: before = 126662 after = 126421 dropped = 241\n",
      "2023: before = 125789 after = 125671 dropped = 118\n",
      "2024: before = 125187 after = 125187 dropped = 0\n"
     ]
    }
   ],
   "source": [
    "# Cleaning missing values\n",
    "rows_before = len(usagers_2019)\n",
    "mask_minus1 = (usagers_2019[\"grav\"] == -1) | (usagers_2019[\"an_nais\"] == -1)\n",
    "usagers_2019_clean = usagers_2019.loc[~mask_minus1].copy()\n",
    "print(\"2019:\", \"before =\", rows_before, \"after =\", len(usagers_2019_clean), \"dropped =\", int(mask_minus1.sum()))\n",
    "# 2019: before = 132977 after = 132977 dropped = 0\n",
    "# same for 2020 - 2024\n",
    "rows_before = len(usagers_2020)\n",
    "mask_minus1 = (usagers_2020[\"grav\"] == -1) | (usagers_2020[\"an_nais\"] == -1)\n",
    "usagers_2020_clean = usagers_2020.loc[~mask_minus1].copy()\n",
    "print(\"2020:\", \"before =\", rows_before, \"after =\", len(usagers_2020_clean), \"dropped =\", int(mask_minus1.sum()))\n",
    "rows_before = len(usagers_2021)\n",
    "mask_minus1 = (usagers_2021[\"grav\"] == -1) | (usagers_2021[\"an_nais\"] == -1)\n",
    "usagers_2021_clean = usagers_2021.loc[~mask_minus1].copy()\n",
    "print(\"2021:\", \"before =\", rows_before, \"after =\", len(usagers_2021_clean), \"dropped =\", int(mask_minus1.sum()))\n",
    "rows_before = len(usagers_2022)\n",
    "mask_minus1 = (usagers_2022[\"grav\"] == -1) | (usagers_2022[\"an_nais\"] == -1)\n",
    "usagers_2022_clean = usagers_2022.loc[~mask_minus1].copy()\n",
    "print(\"2022:\", \"before =\", rows_before, \"after =\", len(usagers_2022_clean), \"dropped =\", int(mask_minus1.sum()))\n",
    "rows_before = len(usagers_2023)\n",
    "mask_minus1 = (usagers_2023[\"grav\"] == -1) | (usagers_2023[\"an_nais\"] == -1)\n",
    "usagers_2023_clean = usagers_2023.loc[~mask_minus1].copy()\n",
    "print(\"2023:\", \"before =\", rows_before, \"after =\", len(usagers_2023_clean), \"dropped =\", int(mask_minus1.sum()))\n",
    "rows_before = len(usagers_2024)\n",
    "mask_minus1 = (usagers_2024[\"grav\"] == -1) | (usagers_2024[\"an_nais\"] == -1)\n",
    "usagers_2024_clean = usagers_2024.loc[~mask_minus1].copy()\n",
    "print(\"2024:\", \"before =\", rows_before, \"after =\", len(usagers_2024_clean), \"dropped =\", int(mask_minus1.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf671bd",
   "metadata": {},
   "source": [
    "# Remove exact duplicate rows per year\n",
    "- drop exact duplicate rows within each yearly cleaned dataframe.\n",
    "- print out shows how many rows were dropped per year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "208dbef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 duplicates removed = 101 rows_after = 132876\n",
      "2020 duplicates removed = 63 rows_after = 105232\n",
      "2021 duplicates removed = 0 rows_after = 129188\n",
      "2022 duplicates removed = 0 rows_after = 126421\n",
      "2023 duplicates removed = 0 rows_after = 125671\n",
      "2024 duplicates removed = 0 rows_after = 125187\n"
     ]
    }
   ],
   "source": [
    "\n",
    "before = len(usagers_2019_clean)\n",
    "usagers_2019_nodup = usagers_2019_clean.drop_duplicates(keep=\"first\").copy()\n",
    "removed = before - len(usagers_2019_nodup)\n",
    "print(\"2019 duplicates removed =\", removed, \"rows_after =\", len(usagers_2019_nodup))\n",
    "before = len(usagers_2020_clean)\n",
    "usagers_2020_nodup = usagers_2020_clean.drop_duplicates(keep=\"first\").copy()\n",
    "removed = before - len(usagers_2020_nodup)\n",
    "print(\"2020 duplicates removed =\", removed, \"rows_after =\", len(usagers_2020_nodup))\n",
    "before = len(usagers_2021_clean)\n",
    "usagers_2021_nodup = usagers_2021_clean.drop_duplicates(keep=\"first\").copy()\n",
    "removed = before - len(usagers_2021_nodup)\n",
    "print(\"2021 duplicates removed =\", removed, \"rows_after =\", len(usagers_2021_nodup))\n",
    "before = len(usagers_2022_clean)\n",
    "usagers_2022_nodup = usagers_2022_clean.drop_duplicates(keep=\"first\").copy()\n",
    "removed = before - len(usagers_2022_nodup)\n",
    "print(\"2022 duplicates removed =\", removed, \"rows_after =\", len(usagers_2022_nodup))\n",
    "before = len(usagers_2023_clean)\n",
    "usagers_2023_nodup = usagers_2023_clean.drop_duplicates(keep=\"first\").copy()\n",
    "removed = before - len(usagers_2023_nodup)\n",
    "print(\"2023 duplicates removed =\", removed, \"rows_after =\", len(usagers_2023_nodup))\n",
    "before = len(usagers_2024_clean)\n",
    "usagers_2024_nodup = usagers_2024_clean.drop_duplicates(keep=\"first\").copy()\n",
    "removed = before - len(usagers_2024_nodup)\n",
    "print(\"2024 duplicates removed =\", removed, \"rows_after =\", len(usagers_2024_nodup))\n",
    "#2019 duplicates removed = 101 rows_after = 132876\n",
    "#2020 duplicates removed = 63 rows_after = 105232\n",
    "#2021 duplicates removed = 0 rows_after = 129188\n",
    "#2022 duplicates removed = 0 rows_after = 126421\n",
    "#2023 duplicates removed = 0 rows_after = 125671\n",
    "#2024 duplicates removed = 0 rows_after = 125187"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca4972d",
   "metadata": {},
   "source": [
    "# add year-variable and compute age-variable, harmonize 'grav'-codes(severity)\n",
    "- year is necessary for compuation of 'age' and to make temporal provenance explicit\n",
    "- Compute `age` from `an_nais` using `age = year - an_nais` and coerce invalid birthyears (<1900 or > year) to missing.(these values are not logical) Store `age` as a nullable integer.\n",
    "- Harmonize severity codes by swapping values 2 and 4 (4=killed and 2=light injured). This supports subsequent interpretation and gives the codes of the target variables a logical increase. 0 lowest severity to 4 highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6676e97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 125187 entries, 0 to 125186\n",
      "Series name: age\n",
      "Non-Null Count   Dtype\n",
      "--------------   -----\n",
      "122608 non-null  Int64\n",
      "dtypes: Int64(1)\n",
      "memory usage: 2.0 MB\n",
      "None\n",
      "2019 before: {1: 55271, 2: 53256, 3: 20852, 4: 3497}\n",
      "2019 after: {1: 55271, 4: 53256, 3: 20852, 2: 3497}\n",
      "2020 before: {1: 43229, 2: 42451, 3: 16772, 4: 2780}\n",
      "2020 after: {1: 43229, 4: 42451, 3: 16772, 2: 2780}\n",
      "2021 before: {1: 55143, 2: 51733, 3: 19093, 4: 3219}\n",
      "2021 after: {1: 55143, 4: 51733, 3: 19093, 2: 3219}\n",
      "2022 before: {1: 53630, 2: 49981, 3: 19260, 4: 3550}\n",
      "2022 after: {1: 53630, 4: 49981, 3: 19260, 2: 3550}\n",
      "2023 before: {1: 53399, 2: 49603, 3: 19271, 4: 3398}\n",
      "2023 after: {1: 53399, 4: 49603, 3: 19271, 2: 3398}\n",
      "2024 before: {1: 52920, 2: 49709, 3: 19126, 4: 3432}\n",
      "2024 after: {1: 52920, 4: 49709, 3: 19126, 2: 3432}\n"
     ]
    }
   ],
   "source": [
    "# creation of \"year\"\n",
    "usagers_2019_nodup[\"year\"] = 2019\n",
    "usagers_2020_nodup[\"year\"] = 2020\n",
    "usagers_2021_nodup[\"year\"] = 2021\n",
    "usagers_2022_nodup[\"year\"] = 2022\n",
    "usagers_2023_nodup[\"year\"] = 2023\n",
    "usagers_2024_nodup[\"year\"] = 2024\n",
    "\n",
    "# create function for compute age and create 'age' for each year\n",
    "def compute_age(df):\n",
    "    year =df['year'].iloc[0]\n",
    "    an_nais = df.get('an_nais')\n",
    "    age =(year -an_nais).where(an_nais.notna(),np.nan)\n",
    "    invalid=(an_nais <1900) | (an_nais >year)\n",
    "    age =age.where(~invalid,np.nan)\n",
    "    df.loc[:,'age']=age.astype('Int64')\n",
    "    return df\n",
    "usagers_2019_nodup = compute_age(usagers_2019_nodup)\n",
    "usagers_2020_nodup = compute_age(usagers_2020_nodup)\n",
    "usagers_2021_nodup = compute_age(usagers_2021_nodup)\n",
    "usagers_2022_nodup = compute_age(usagers_2022_nodup)\n",
    "usagers_2023_nodup = compute_age(usagers_2023_nodup)\n",
    "usagers_2024_nodup = compute_age(usagers_2024_nodup)\n",
    "print(usagers_2024_nodup['age'].info())\n",
    "# swap grav 2 and 4 in each datafram \n",
    "map_swap = {2: 4, 4: 2}\n",
    "for df, name in [(usagers_2019_nodup, \"2019\"), (usagers_2020_nodup, \"2020\"), (usagers_2021_nodup, \"2021\"),\n",
    "                 (usagers_2022_nodup, \"2022\"), (usagers_2023_nodup, \"2023\"), (usagers_2024_nodup, \"2024\")]:\n",
    "    print(name, \"before:\", df[\"grav\"].value_counts(dropna=False).to_dict())\n",
    "    df.loc[:, \"grav\"] = df[\"grav\"].map(lambda v: map_swap.get(v, v))\n",
    "    print(name, \"after:\", df[\"grav\"].value_counts(dropna=False).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21366d8e",
   "metadata": {},
   "source": [
    "# Rename columns, harmonize schema, concatenate per-year frames into usagers_final and save snapshot\n",
    "- rename original French columns to consistent english identifiers\n",
    "- check that indivual_age (not existent before 2021) is set to missing value\n",
    "- drop individual_birthyear to reduce data size and because Age stores this information now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2afcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (744575, 18)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 744575 entries, 0 to 744574\n",
      "Data columns (total 18 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   acc_num                   744575 non-null  int64  \n",
      " 1   individual_vehID          744575 non-null  object \n",
      " 2   veh_num                   744575 non-null  object \n",
      " 3   individual_place          744575 non-null  int64  \n",
      " 4   individual_cat            744575 non-null  int64  \n",
      " 5   individual_severity       744575 non-null  int64  \n",
      " 6   individual_sex            744575 non-null  int64  \n",
      " 7   individual_birthyear      733875 non-null  float64\n",
      " 8   individual_trip           744575 non-null  int64  \n",
      " 9   individual_secu1          744575 non-null  int64  \n",
      " 10  individual_secu2          744575 non-null  int64  \n",
      " 11  user_secu3                744575 non-null  int64  \n",
      " 12  individual_location       744575 non-null  int64  \n",
      " 13  individual_action         744575 non-null  object \n",
      " 14  individual_companionship  744575 non-null  int64  \n",
      " 15  year                      744575 non-null  int64  \n",
      " 16  age                       733875 non-null  Int64  \n",
      " 17  individual_id             506467 non-null  object \n",
      "dtypes: Int64(1), float64(1), int64(12), object(4)\n",
      "memory usage: 103.0+ MB\n",
      "None\n",
      "        acc_num individual_vehID veh_num  individual_place  individual_cat  \\\n",
      "0  201900000001      138 306 524     B01                 2               2   \n",
      "1  201900000001      138 306 524     B01                 1               1   \n",
      "2  201900000001      138 306 525     A01                 1               1   \n",
      "3  201900000002      138 306 523     A01                 1               1   \n",
      "4  201900000003      138 306 520     A01                 1               1   \n",
      "\n",
      "   individual_severity  individual_sex  individual_birthyear  individual_trip  \\\n",
      "0                    4               2                2002.0                0   \n",
      "1                    4               2                1993.0                5   \n",
      "2                    1               1                1959.0                0   \n",
      "3                    4               2                1994.0                0   \n",
      "4                    1               1                1996.0                0   \n",
      "\n",
      "   individual_secu1  individual_secu2  user_secu3  individual_location  \\\n",
      "0                 1                 0          -1                   -1   \n",
      "1                 1                 0          -1                   -1   \n",
      "2                 1                 0          -1                   -1   \n",
      "3                 1                 0          -1                   -1   \n",
      "4                 1                 0          -1                   -1   \n",
      "\n",
      "  individual_action  individual_companionship  year  age individual_id  \n",
      "0                -1                        -1  2019   17           NaN  \n",
      "1                -1                        -1  2019   26           NaN  \n",
      "2                -1                        -1  2019   60           NaN  \n",
      "3                -1                        -1  2019   25           NaN  \n",
      "4                 0                        -1  2019   23           NaN  \n",
      "             acc_num individual_vehID veh_num  individual_place  \\\n",
      "count   7.445750e+05           744575  744575     744575.000000   \n",
      "unique           NaN           555558      86               NaN   \n",
      "top              NaN      155 602 779     A01               NaN   \n",
      "freq             NaN               65  448809               NaN   \n",
      "mean    2.021514e+11              NaN     NaN          2.100597   \n",
      "std     1.717437e+08              NaN     NaN          2.596966   \n",
      "min     2.019000e+11              NaN     NaN         -1.000000   \n",
      "25%     2.020000e+11              NaN     NaN          1.000000   \n",
      "50%     2.022000e+11              NaN     NaN          1.000000   \n",
      "75%     2.023000e+11              NaN     NaN          2.000000   \n",
      "max     2.024001e+11              NaN     NaN         10.000000   \n",
      "\n",
      "        individual_cat  individual_severity  individual_sex  \\\n",
      "count    744575.000000        744575.000000   744575.000000   \n",
      "unique             NaN                  NaN             NaN   \n",
      "top                NaN                  NaN             NaN   \n",
      "freq               NaN                  NaN             NaN   \n",
      "mean          1.334659             2.529494        1.285077   \n",
      "std           0.614004             1.374236        0.534913   \n",
      "min           1.000000             1.000000       -1.000000   \n",
      "25%           1.000000             1.000000        1.000000   \n",
      "50%           1.000000             3.000000        1.000000   \n",
      "75%           2.000000             4.000000        2.000000   \n",
      "max           3.000000             4.000000        2.000000   \n",
      "\n",
      "        individual_birthyear  individual_trip  individual_secu1  \\\n",
      "count          733875.000000    744575.000000     744575.000000   \n",
      "unique                   NaN              NaN               NaN   \n",
      "top                      NaN              NaN               NaN   \n",
      "freq                     NaN              NaN               NaN   \n",
      "mean             1982.912766         3.175068          1.943311   \n",
      "std                19.089213         2.750914          2.342639   \n",
      "min              1900.000000        -1.000000         -1.000000   \n",
      "25%              1970.000000         0.000000          1.000000   \n",
      "50%              1986.000000         4.000000          1.000000   \n",
      "75%              1998.000000         5.000000          2.000000   \n",
      "max              2024.000000         9.000000          9.000000   \n",
      "\n",
      "        individual_secu2     user_secu3  individual_location  \\\n",
      "count      744575.000000  744575.000000        744575.000000   \n",
      "unique               NaN            NaN                  NaN   \n",
      "top                  NaN            NaN                  NaN   \n",
      "freq                 NaN            NaN                  NaN   \n",
      "mean            1.038844      -0.891326            -0.203000   \n",
      "std             3.070058       0.940800             1.239422   \n",
      "min            -1.000000      -1.000000            -1.000000   \n",
      "25%            -1.000000      -1.000000            -1.000000   \n",
      "50%             0.000000      -1.000000             0.000000   \n",
      "75%             0.000000      -1.000000             0.000000   \n",
      "max             9.000000       9.000000             9.000000   \n",
      "\n",
      "       individual_action  individual_companionship           year        age  \\\n",
      "count             744575             744575.000000  744575.000000   733875.0   \n",
      "unique                13                       NaN            NaN       <NA>   \n",
      "top                    0                       NaN            NaN       <NA>   \n",
      "freq              376021                       NaN            NaN       <NA>   \n",
      "mean                 NaN                 -0.821915    2021.513501  38.587332   \n",
      "std                  NaN                  0.628932       1.717439  19.017614   \n",
      "min                  NaN                 -1.000000    2019.000000        0.0   \n",
      "25%                  NaN                 -1.000000    2020.000000       23.0   \n",
      "50%                  NaN                 -1.000000    2022.000000       35.0   \n",
      "75%                  NaN                 -1.000000    2023.000000       52.0   \n",
      "max                  NaN                  3.000000    2024.000000      120.0   \n",
      "\n",
      "       individual_id  \n",
      "count         506467  \n",
      "unique        506467  \n",
      "top          267 638  \n",
      "freq               1  \n",
      "mean             NaN  \n",
      "std              NaN  \n",
      "min              NaN  \n",
      "25%              NaN  \n",
      "50%              NaN  \n",
      "75%              NaN  \n",
      "max              NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 744575 entries, 0 to 744574\n",
      "Data columns (total 17 columns):\n",
      " #   Column                    Non-Null Count   Dtype \n",
      "---  ------                    --------------   ----- \n",
      " 0   acc_num                   744575 non-null  int64 \n",
      " 1   individual_vehID          744575 non-null  object\n",
      " 2   veh_num                   744575 non-null  object\n",
      " 3   individual_place          744575 non-null  int64 \n",
      " 4   individual_cat            744575 non-null  int64 \n",
      " 5   individual_severity       744575 non-null  int64 \n",
      " 6   individual_sex            744575 non-null  int64 \n",
      " 7   individual_trip           744575 non-null  int64 \n",
      " 8   individual_secu1          744575 non-null  int64 \n",
      " 9   individual_secu2          744575 non-null  int64 \n",
      " 10  user_secu3                744575 non-null  int64 \n",
      " 11  individual_location       744575 non-null  int64 \n",
      " 12  individual_action         744575 non-null  object\n",
      " 13  individual_companionship  744575 non-null  int64 \n",
      " 14  year                      744575 non-null  int64 \n",
      " 15  age                       733875 non-null  Int64 \n",
      " 16  individual_id             506467 non-null  object\n",
      "dtypes: Int64(1), int64(12), object(4)\n",
      "memory usage: 97.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rename_map = {\n",
    "    \"Num_Acc\":\"acc_num\",\n",
    "    \"place\":\"individual_place\",\n",
    "    \"catu\":\"individual_cat\",\n",
    "    \"grav\":\"individual_severity\",\n",
    "    \"sexe\":\"individual_sex\",\n",
    "    \"trajet\":\"individual_trip\",\n",
    "    \"secu\":\"individual_safety\",\n",
    "    \"locp\":\"individual_location\",\n",
    "    \"actp\":\"individual_action\",\n",
    "    \"etatp\":\"individual_companionship\",\n",
    "    \"an_nais\":\"individual_birthyear\",\n",
    "    \"num_veh\":\"veh_num\",\n",
    "    \"id_vehicule\":\"individual_vehID\",\n",
    "    \"secu1\":\"individual_secu1\",\n",
    "    \"secu2\":\"individual_secu2\",\n",
    "    \"secu3\":\"user_secu3\",\n",
    "    \"id_usager\":\"individual_id\"\n",
    "}\n",
    "dfs=[usagers_2019_nodup, usagers_2020_nodup, usagers_2021_nodup,\n",
    "     usagers_2022_nodup, usagers_2023_nodup, usagers_2024_nodup]\n",
    "for df in dfs:\n",
    "    # rename only for existing columns\n",
    "    df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns}, inplace=True)\n",
    "    # missing ids > no exist before 2021\n",
    "    if \"individual_id\" not in df.columns:\n",
    "        df.loc[:, \"individual_id\"] = pd.NA\n",
    "    # ensure correct dtypes\n",
    "    if \"individual_birthyear\" in df.columns:\n",
    "        df.loc[:, \"individual_birthyear\"] = pd.to_numeric(df[\"individual_birthyear\"], errors=\"coerce\").astype(\"float\")\n",
    "    df.loc[:, \"age\"] = pd.to_numeric(df.get(\"age\"), errors=\"coerce\").astype(\"float\")\n",
    "\n",
    "usagers_final = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Final shape:\", usagers_final.shape)\n",
    "print(usagers_final.info())\n",
    "print(usagers_final.head())\n",
    "print(usagers_final.describe(include='all'))\n",
    "usagers_final=usagers_final.drop('individual_birthyear',axis=1) # dropoing birthyear as age is present and to reduce size of dataset for upload\n",
    "joblib.dump(usagers_final, \"1.0-becker-data-preprocessing_usagers\")\n",
    "\n",
    "print(usagers_final.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590a560",
   "metadata": {},
   "source": [
    "# check rows before/after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b49863b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum per-year rows = 744575 usagers_final rows = 744575\n"
     ]
    }
   ],
   "source": [
    "sum_rows = sum(len(df) for df in [usagers_2019_nodup, usagers_2020_nodup, usagers_2021_nodup,\n",
    "                                  usagers_2022_nodup, usagers_2023_nodup, usagers_2024_nodup])\n",
    "print(\"sum per-year rows =\", sum_rows, \"usagers_final rows =\", len(usagers_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece68b3",
   "metadata": {},
   "source": [
    "# possible next steps for created dataset\n",
    "- rename columns to ind_column\n",
    "- to discuss for further steps: \n",
    "    - Categorical level reduction & encoding policy:\n",
    "    > secu- columns. exploration showed that there are some categories which have non-significant appearance in all years\n",
    "    > reduce categories 5,6,7 could be aggregated to one new category. \n",
    "    > age-variable could be for example groups of aggreagated ages (0-17,18-24,25-44,45-64,65+)\n",
    "    > create binary target variable (serious/non-serious) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
