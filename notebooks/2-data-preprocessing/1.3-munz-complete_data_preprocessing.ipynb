{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb285c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "author: Michael Munz\n",
    "\n",
    "Use pipeline in Jupyter notebook to streamline data processing.\n",
    "Follow these steps:\n",
    "- import pipeline factory functions\n",
    "- load raw data into pd.DataFrame\n",
    "- init pipeline\n",
    "- fit & transform data via fit_transform()\n",
    "- transform new data (:validation_set, :test_set) via transform() to\n",
    "apply same preprocessing steps based on training fit.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4e077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append( '../../library' )\n",
    "import gc_storage as gcs\n",
    "import data_preprocessing_utils as dpu\n",
    "import data_preprocessing_pipeline as dpp\n",
    "\n",
    "from joblib import load\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf31add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init google cloud storage (GCS)\n",
    "bucket_name='sep25-bds-road-accidents'\n",
    "key_path='../../auth/fiery-glass-478009-t8-18a81c8cbe63.json'\n",
    "\n",
    "bucket = gcs.init_bucket( bucket=bucket_name,\n",
    "                          json_key_path=key_path )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in GCS\n",
    "gcs.list_bucket( bucket=bucket,\n",
    "                 remote_folder='2_preprocessing' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57faf485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from GCS\n",
    "df = gcs.download( bucket=bucket,\n",
    "                   remote_path='2_preprocessing/1.0-leibold-data-preprocessing_aggr.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5f4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = load( '../../data/processed/2_preprocessing/1.0-leibold-data-preprocessing_aggr.gc' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration & pipeline config for ML pre-processing\n",
    "\n",
    "# get ALL column categories filtered to EXISTING cols\n",
    "col_categories = dpu.categorize_dataframe_columns(df)\n",
    "\n",
    "# create summary table\n",
    "dpu.display_column_categories( col_categories )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76df1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target distribution:\n",
      "ind_severity\n",
      "1    285859\n",
      "2    280987\n",
      "3    106958\n",
      "4     18355\n",
      "Name: count, dtype: int64\n",
      "\n",
      "target distribution:\n",
      "ind_severity\n",
      "1    0.413\n",
      "2    0.406\n",
      "3    0.155\n",
      "4    0.027\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# explanatory vars :X\n",
    "X = df.drop( columns='ind_severity',\n",
    "             axis=1 )\n",
    "\n",
    "# target var y: :ind_severity\n",
    "# important: var is unbalanced\n",
    "y = df.ind_severity\n",
    "\n",
    "print( f\"target distribution:\\n{y.value_counts()}\\n\" )\n",
    "print( f\"target distribution:\\n{y.value_counts(normalize=True).round(3)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22693d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (484511, 44)\n",
      "test shape: (207648, 44)\n"
     ]
    }
   ],
   "source": [
    "# data splitting with stratification\n",
    "# split into training set, test set BEFORE applying pipeline + resampling\n",
    "# stratify=y -> stratified split; prevents bias\n",
    "#               ensures class distribution (proportions of each target class)\n",
    "#               in { :y_train, :y_test } matches original :y\n",
    "# stratify=n -> random split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, \n",
    "                                                     y, \n",
    "                                                     test_size=0.3, \n",
    "                                                     random_state=369, \n",
    "                                                     stratify=y )\n",
    "\n",
    "print( f\"train shape: {X_train.shape}\" )\n",
    "print( f\"test shape: {X_test.shape}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration & pipeline config for ML pre-processing\n",
    "\n",
    "# get ALL column categories filtered to EXISTING cols\n",
    "X_train_col_categories = dpu.categorize_dataframe_columns( X_train )\n",
    "\n",
    "# create summary table\n",
    "dpu.display_column_categories( X_train_col_categories )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f483e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (200, 79), indices imply (200, 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     11\u001b[39m X_sample = X_train.sample( \u001b[32m200\u001b[39m,\n\u001b[32m     12\u001b[39m                            random_state=\u001b[32m369\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# :get_feature_names_out()\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# # opt 1 - manual\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# preprocessor = pipeline.named_steps[ 'preparation' ]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# opt 2 - via method\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m feature_names = \u001b[43mdpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_full_feature_names_from_preprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m                                                              \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# inspect\u001b[39;00m\n\u001b[32m     32\u001b[39m display( \u001b[33m\"\u001b[39m\u001b[33mNo. of features: \u001b[39m\u001b[33m{\u001b[39m\u001b[33mlen(feature_names)}\u001b[39m\u001b[33m\"\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/SEP25-BDS-Road-Accidents/notebooks/2-data-preprocessing/../../library/data_preprocessing_pipeline.py:1537\u001b[39m, in \u001b[36mget_full_feature_names_from_preprocessor\u001b[39m\u001b[34m(X, preprocessor)\u001b[39m\n\u001b[32m   1535\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1536\u001b[39m         feature_names = step.get_feature_names_out(X_work.columns)\n\u001b[32m-> \u001b[39m\u001b[32m1537\u001b[39m     X_work = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_work\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1539\u001b[39m     X_work = step.transform(X_work)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/pandas/core/frame.py:831\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    820\u001b[39m         mgr = dict_to_mgr(\n\u001b[32m    821\u001b[39m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[32m    822\u001b[39m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    828\u001b[39m             copy=_copy,\n\u001b[32m    829\u001b[39m         )\n\u001b[32m    830\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/pandas/core/internals/construction.py:336\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[32m    332\u001b[39m index, columns = _get_axes(\n\u001b[32m    333\u001b[39m     values.shape[\u001b[32m0\u001b[39m], values.shape[\u001b[32m1\u001b[39m], index=index, columns=columns\n\u001b[32m    334\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/pandas/core/internals/construction.py:420\u001b[39m, in \u001b[36m_check_values_indices_shape_match\u001b[39m\u001b[34m(values, index, columns)\u001b[39m\n\u001b[32m    418\u001b[39m passed = values.shape\n\u001b[32m    419\u001b[39m implied = (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Shape of passed values is (200, 79), indices imply (200, 77)"
     ]
    }
   ],
   "source": [
    "# using small sample of training set\n",
    "pipeline = dpp.build_default_full_pipeline()\n",
    "\n",
    "# fit full pipeline :X_train, :y_train\n",
    "pipeline.fit( X_train,\n",
    "              y_train )\n",
    "\n",
    "preprocessor = pipeline.named_steps['preparation']\n",
    "\n",
    "# use small sample\n",
    "X_sample = X_train.sample( 200,\n",
    "                           random_state=369 )\n",
    "\n",
    "# :get_feature_names_out()\n",
    "# # opt 1 - manual\n",
    "# preprocessor = pipeline.named_steps[ 'preparation' ]\n",
    "\n",
    "# # last step (ColumnTransformer)\n",
    "# encoding = preprocessor.named_steps[ 'encoding' ]\n",
    "\n",
    "# feature_names = encoding.get_feature_names_out()\n",
    "\n",
    "# print( len(feature_names) )\n",
    "# print( feature_names[:10] )\n",
    "\n",
    "# opt 2 - via method\n",
    "feature_names = dpp.get_full_feature_names_from_preprocessor( X=X_sample,\n",
    "                                                              preprocessor=preprocessor )\n",
    "\n",
    "\n",
    "# inspect\n",
    "display( \"No. of features: {len(feature_names)}\" )\n",
    "display( feature_names[:10] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf79189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m X_train_processed = pipeline.transform( X_train )\n\u001b[32m      3\u001b[39m X_train_processed_df = pd.DataFrame( X_train_processed,\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m                                      columns=\u001b[43mfeature_names\u001b[49m,\n\u001b[32m      5\u001b[39m                                      index=X_train.index )\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m( X_train_processed_df.shape )\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m( X_train_processed_df.head() )\n",
      "\u001b[31mNameError\u001b[39m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_processed = pipeline.transform( X_train )\n",
    "\n",
    "X_train_processed_df = pd.DataFrame( X_train_processed,\n",
    "                                     columns=feature_names,\n",
    "                                     index=X_train.index )\n",
    "\n",
    "print( X_train_processed_df.shape )\n",
    "print( X_train_processed_df.head() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a383cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build fully configured pipeline with defaults\n",
    "pipeline = dpp.build_default_full_pipeline()\n",
    "pipeline.fit( X_train,\n",
    "              y_train )\n",
    "\n",
    "X_train_processed = pipeline.transform( X_train )\n",
    "\n",
    "n_features = X_train_processed.shape[1]\n",
    "\n",
    "feature_names = [ f\"{i}\" for i in range(n_features) ]\n",
    "\n",
    "X_train_processed_df = pd.DataFrame(\n",
    "    X_train_processed,\n",
    "    columns=feature_names,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "print( f\"processed train shape: {X_train_processed.shape}\" )\n",
    "display( X_train_processed_df.head() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display( X_train_processed_df.head() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train baseline model\n",
    "# opt 1: use processed data directly with simple model to validate pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=369,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 1 apply fit\n",
    "clf.fit( X_train_processed,\n",
    "         y_train )\n",
    "\n",
    "# 2 make prediction\n",
    "y_pred = clf.predict( X_test_processed )\n",
    "\n",
    "# 3 classification report\n",
    "print( classification_report(y_test,\n",
    "                             y_pred) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832396ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt 2: wrap model + preprocessing into single pipeline\n",
    "preprocessing_pipeline = dpp.build_default_full_pipeline()\n",
    "\n",
    "# single pipeline\n",
    "model_rfc = Pipeline([\n",
    "    ( 'preprocessing',\n",
    "      preprocessing_pipeline ),\n",
    "    ( 'model',\n",
    "      RandomForestClassifier(n_estimators=200,\n",
    "                             random_state=369,\n",
    "                             n_jobs=-1) )\n",
    "])\n",
    "\n",
    "# apply fit on :X_train, :y_train\n",
    "model_rfc.fit( X_train,\n",
    "               y_train )\n",
    "\n",
    "# make prediction on :X_test\n",
    "y_pred = model_rfc.predict( X_test )\n",
    "\n",
    "# classification report\n",
    "print( classification_report(y_test, \n",
    "                             y_pred) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55539630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist pre-processed :training_set, :test_set\n",
    "\n",
    "# :X_train_processed\n",
    "gcs.upload( bucket=bucket,\n",
    "            obj=X_train_processed,\n",
    "            local_folder='2_preprocessing',\n",
    "            file_name='1.0-munz-preprocessing-X_train_processed.gc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :y_train\n",
    "gcs.upload( bucket=bucket,\n",
    "            obj=y_train,\n",
    "            local_folder='2_preprocessing',\n",
    "            file_name='1.0-munz-preprocessing-y_train.gc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :X_test_processed\n",
    "gcs.upload( bucket=bucket,\n",
    "            obj=X_test_processed,\n",
    "            local_folder='2_preprocessing',\n",
    "            file_name='1.0-munz-preprocessing-X_test_processed.gc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa822a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :y_test\n",
    "gcs.upload( bucket=bucket,\n",
    "            obj=y_test,\n",
    "            local_folder='2_preprocessing',\n",
    "            file_name='1.0-munz-preprocessing-y_test.gc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
