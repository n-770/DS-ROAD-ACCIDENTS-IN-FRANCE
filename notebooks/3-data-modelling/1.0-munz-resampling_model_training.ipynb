{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb20e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nauthor: Michael Munz\\n\\nworkflow applying re-sampling and avoiding data leakage use imblearn (Imbalanced-learn)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "author: Michael Munz\n",
    "\n",
    "workflow applying re-sampling and avoiding data leakage use imblearn (Imbalanced-learn)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4204dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append( '../../library' )\n",
    "import gc_storage as gcs\n",
    "import data_preprocessing_pipeline as dpp\n",
    "\n",
    "from joblib import load\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized sep25-bds-road-accidents\n"
     ]
    }
   ],
   "source": [
    "# init GCS\n",
    "bucket_name='sep25-bds-road-accidents'\n",
    "key_path='../../auth/fiery-glass-478009-t8-18a81c8cbe63.json'\n",
    "\n",
    "bucket = gcs.init_bucket( bucket=bucket_name,\n",
    "                          json_key_path=key_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35332e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blobs: [34]\n",
      "data/processed/2_preprocessing/\n",
      "data/processed/2_preprocessing/0.1-munz-data-preprocessing_locations.joblib\n",
      "data/processed/2_preprocessing/0.2-munz-data-preprocessing_locations.joblib\n",
      "data/processed/2_preprocessing/0.3-munz-data-preprocessing_locations.joblib\n",
      "data/processed/2_preprocessing/0.4-munz-data-preprocessing_locations.joblib\n",
      "data/processed/2_preprocessing/1.0-becker-data-preprocessing_usagers.joblib\n",
      "data/processed/2_preprocessing/1.0-leibold-data-preprocessing_aggr.joblib\n",
      "data/processed/2_preprocessing/1.0-leibold-data-preprocessing_vehicles.joblib\n",
      "data/processed/2_preprocessing/1.0-munz-acc-municipality_X_test_uniques_lookup_table.gc\n",
      "data/processed/2_preprocessing/1.0-munz-acc-municipality_X_train_uniques_lookup_table.gc\n",
      "data/processed/2_preprocessing/1.0-munz-preprocessing-X_test.gc\n",
      "data/processed/2_preprocessing/1.0-munz-preprocessing-X_test_processed.gc\n",
      "data/processed/2_preprocessing/1.0-munz-preprocessing-X_train_num.gc\n",
      "data/processed/2_preprocessing/1.0-munz-preprocessing-X_train_processed.gc\n",
      "data/processed/2_preprocessing/1.0-munz-preprocessing-y_test.gc\n",
      "data/processed/2_preprocessing/1.0-munz-preprocessing-y_train.gc\n",
      "data/processed/2_preprocessing/1.0-simmler-data-preprocessing_accidents.joblib\n",
      "data/processed/2_preprocessing/1.0.1-munz-data-preprocessing_locations.joblib\n",
      "data/processed/2_preprocessing/1.0.2-munz-data-preprocessing_locations.joblib\n",
      "data/processed/2_preprocessing/1.0.3-munz-data-preprocessing_locations.joblib\n",
      "data/processed/2_preprocessing/1.1-becker-data-preprocessing_usagers.joblib\n",
      "data/processed/2_preprocessing/1.1-leibold-data-preprocessing_concat.joblib\n",
      "data/processed/2_preprocessing/1.1-leibold-data-preprocessing_vehicles.joblib\n",
      "data/processed/2_preprocessing/1.1-munz-data-preprocessing_locations.joblib\n",
      "data/processed/2_preprocessing/1.1-simmler-data-preprocessing_accidents.joblib\n",
      "data/processed/2_preprocessing/1.1-simmler-data-preprocessing_accidents_geo_grouped.joblib\n",
      "data/processed/2_preprocessing/1.1-simmler-data-preprocessing_accidents_severity.joblib\n",
      "data/processed/2_preprocessing/1.1-simmler-data-preprocessing_accidents_severity_r.joblib\n",
      "data/processed/2_preprocessing/1.1-simmler-data-preprocessing_accidents_severity_reordered.joblib\n",
      "data/processed/2_preprocessing/1.2-becker-data-preprocessing_usagers.joblib\n",
      "data/processed/2_preprocessing/1.2-leibold-data-preprocessing_concat.joblib\n",
      "data/processed/2_preprocessing/1.2-leibold-data-preprocessing_vehicles.joblib\n",
      "data/processed/2_preprocessing/1.3-becker-data-preprocessing_usagers.joblib\n",
      "data/processed/2_preprocessing/1.3-leibold-data-preprocessing_vehicles.joblib\n"
     ]
    }
   ],
   "source": [
    "# listing all blobs in GCS\n",
    "gcs.list_bucket( bucket=bucket,\n",
    "                 remote_folder='2_preprocessing' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7546fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = load( '../../data/processed/2_preprocessing/1.0-leibold-data-preprocessing_aggr.gc' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab01b0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target distribution:\n",
      "ind_severity\n",
      "1    285859\n",
      "2    280987\n",
      "3    106958\n",
      "4     18355\n",
      "Name: count, dtype: int64\n",
      "\n",
      "target distribution:\n",
      "ind_severity\n",
      "1    0.413\n",
      "2    0.406\n",
      "3    0.155\n",
      "4    0.027\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1 split features / target\n",
    "\n",
    "# explanatory vars :X\n",
    "X = df.drop( columns='ind_severity',\n",
    "             axis=1 )\n",
    "\n",
    "# target var y: :ind_severity\n",
    "# important: var is unbalanced\n",
    "y = df.ind_severity\n",
    "\n",
    "print( f\"target distribution:\\n{y.value_counts()}\\n\" )\n",
    "print( f\"target distribution:\\n{y.value_counts(normalize=True).round(3)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc9131a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (484511, 44)\n",
      "test shape: (207648, 44)\n"
     ]
    }
   ],
   "source": [
    "# 2 split { :training_set, :test_set }\n",
    "# data splitting with stratification\n",
    "# split into training set, test set BEFORE applying pipeline + resampling\n",
    "# stratify=y -> stratified split; prevents bias\n",
    "#               ensures class distribution (proportions of each target class)\n",
    "#               in { :y_train, :y_test } matches original :y\n",
    "# stratify=n -> random split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, \n",
    "                                                     y, \n",
    "                                                     test_size=0.3, \n",
    "                                                     random_state=369, \n",
    "                                                     stratify=y )\n",
    "\n",
    "print( f\"train shape: {X_train.shape}\" )\n",
    "print( f\"test shape: {X_test.shape}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9f7e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/honolulu/workspace/SEP25-BDS-Road-Accidents/notebooks/3-data-modelling/../../library/data_preprocessing_utils.py:426: UserWarning: ⚠️ The following columns were not found and skipped: ['acc_date', 'acc_department', 'acc_long', 'acc_lat', 'acc_metro', 'ind_action', 'ind_age', 'ind_location', 'ind_secu2', 'ind_year', 'veh_id']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in acc_year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/honolulu/workspace/SEP25-BDS-Road-Accidents/notebooks/3-data-modelling/../../library/data_preprocessing_utils.py:460: UserWarning: ⚠️ Columns not found, skipping: ['loca_road_count']\n",
      "  warnings.warn( f\"⚠️ Columns not found, skipping: {missing_cols}\",\n",
      "/Users/honolulu/workspace/SEP25-BDS-Road-Accidents/notebooks/3-data-modelling/../../library/data_preprocessing_utils.py:758: UserWarning: One-hot encoding: 20 cols → 124 cols (+104)\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in acc_year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/honolulu/workspace/SEP25-BDS-Road-Accidents/notebooks/3-data-modelling/../../library/data_preprocessing_utils.py:460: UserWarning: ⚠️ Columns not found, skipping: ['loca_road_count']\n",
      "  warnings.warn( f\"⚠️ Columns not found, skipping: {missing_cols}\",\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed train shape: (484511, 131)\n",
      "processed test shape: (207648, 131)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/honolulu/workspace/SEP25-BDS-Road-Accidents/notebooks/3-data-modelling/../../library/data_preprocessing_utils.py:758: UserWarning: One-hot encoding: 20 cols → 124 cols (+104)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3 pre-processing pipeline\n",
    "preprocessing_pipeline = dpp.build_default_full_pipeline()\n",
    "\n",
    "# apply fit & transorm on :X_train, :y_train\n",
    "X_train_processed = preprocessing_pipeline.fit_transform( X_train,\n",
    "                                                          y_train )\n",
    "\n",
    "# apply transform on :X_test (same transformations, NO LEAKAGE)\n",
    "X_test_processed = preprocessing_pipeline.transform( X_test )\n",
    "\n",
    "print( f\"processed train shape: {X_train_processed.shape}\" )\n",
    "print( f\"processed test shape: {X_test_processed.shape}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781aed08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.82      0.77     85758\n",
      "           2       0.64      0.68      0.66     84296\n",
      "           3       0.51      0.35      0.42     32087\n",
      "           4       0.38      0.06      0.10      5507\n",
      "\n",
      "    accuracy                           0.67    207648\n",
      "   macro avg       0.57      0.47      0.49    207648\n",
      "weighted avg       0.65      0.67      0.65    207648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 model training\n",
    "# unbalanced\n",
    "model_rfc = RandomForestClassifier( n_estimators=200,\n",
    "                                    random_state=369,\n",
    "                                    n_jobs=-1 )\n",
    "\n",
    "# apply fit on :X_train, :y_train\n",
    "model_rfc.fit( X_train_processed,\n",
    "               y_train )\n",
    "\n",
    "# make prediction on :X_test\n",
    "y_pred = model_rfc.predict( X_test_processed )\n",
    "\n",
    "# classification report\n",
    "print( classification_report(y_test, \n",
    "                             y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e494d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: can only work with pure numeric feature matrices\n",
    "\n",
    "# 1 under-sample majorities\n",
    "# 1 + 2 down-sampling\n",
    "sampling_strategy_majority = {\n",
    "    1: 180_000,\n",
    "    2: 180_000\n",
    "}\n",
    "\n",
    "# 2 over-sample minorities on reduced set\n",
    "# 3 + 4 up-sampling\n",
    "sampling_strategy_minority = {\n",
    "    3: 150_000,\n",
    "    4: 140_000\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c621018",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampler = Pipeline(\n",
    "    steps=[('under', RandomUnderSampler( sampling_strategy=sampling_strategy_majority,\n",
    "                                         random_state=369 )),\n",
    "           ('over', RandomOverSampler( sampling_strategy=sampling_strategy_minority,\n",
    "                                       random_state=369 ))]\n",
    ")\n",
    "\n",
    "# apply :fit_resample on :X_train, :y_train\n",
    "X_rs, y_rs = resampler.fit_resample( X_train_processed,\n",
    "                                     y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e45e5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Training Class Counts:\n",
      "ind_severity\n",
      "1    200101\n",
      "2    196691\n",
      "3     74871\n",
      "4     12848\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training Class Counts: ind_severity\n",
      "1    180000\n",
      "2    180000\n",
      "3    150000\n",
      "4    140000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check new class distribution\n",
    "print( f\"Original Training Class Counts:\\n{ y_train.value_counts() }\\n\" )\n",
    "print( f\"Training Class Counts: { y_rs.value_counts() }\" )\n",
    "\n",
    "# interpretation\n",
    "# over-corrected & over-sampled rarest class (4: :killed)\n",
    "# original class -> heavily imbalanced; majority 1-2\n",
    "# after SMOTEENN -> skewed towards 3-4; classes 1-2 are now minority classes\n",
    "# next steps: trying SMOTE and SMOTE+Tomek to target milder balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2dd423d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize for classes 3-4 -> using GridSearchCV\n",
    "\n",
    "# custom scorer\n",
    "# macro F1 over classes { :3, :4 }\n",
    "def f1_34(y_true, y_pred):\n",
    "    return f1_score( y_true,\n",
    "                     y_pred,\n",
    "                     labels=[3, 4],\n",
    "                     average='macro' )\n",
    " \n",
    "minority_f1 = make_scorer( f1_34 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8014781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "# on resampled data\n",
    "# init :RandomForestClassifier\n",
    "model_rf = RandomForestClassifier( n_jobs=-1,\n",
    "                                   random_state=369 )\n",
    "\n",
    "# params\n",
    "param_grid = {\n",
    "    \"n_estimators\": [300, 600],\n",
    "    \"max_depth\": [None, 20, 10],\n",
    "    \"min_samples_split\": [2, 10],\n",
    "    \"min_samples_leaf\": [1, 5, 10],\n",
    "    \"max_features\": [\"sqrt\", 0.5],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "# :GridSearchCV\n",
    "# focus on classes 3–4\n",
    "model_gscv = GridSearchCV( model_rf,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring=minority_f1,\n",
    "                     cv=3,\n",
    "                     n_jobs=-1,\n",
    "                     verbose=1 )\n",
    "\n",
    "# apply fit on re-sampled training set :X_rs, :y_rs\n",
    "model_gscv.fit( X=X_rs,\n",
    "                y=y_rs )\n",
    "\n",
    "# :best_estimators_\n",
    "best_rf = model_gscv.best_estimator_\n",
    "\n",
    "# predict on :X_test\n",
    "# make prediction -> on original untouched test_set\n",
    "y_pred_gscv = best_rf.predict( X_test_processed )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a930e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.84      0.80     95219\n",
      "           2       0.73      0.65      0.69     93114\n",
      "           3       0.52      0.60      0.56     34672\n",
      "           4       0.53      0.14      0.22      5795\n",
      "\n",
      "    accuracy                           0.71    228800\n",
      "   macro avg       0.64      0.56      0.57    228800\n",
      "weighted avg       0.71      0.71      0.70    228800\n",
      "\n",
      "Confusion Matrix:\n",
      "Predicted Class      1      2      3    4\n",
      "Actual Class                             \n",
      "1                79799  11592   3770   58\n",
      "2                21089  60227  11604  194\n",
      "3                 3772   9790  20633  477\n",
      "4                  504   1027   3441  823\n"
     ]
    }
   ],
   "source": [
    "# model eval\n",
    "# use robust metrics beyond simple accuracy, such as f1-score, precision, recall\n",
    "\n",
    "# classification report\n",
    "print( classification_report(y_test, \n",
    "                             y_pred_gscv) )\n",
    "\n",
    "# confusion matrix\n",
    "cm = pd.crosstab( y_test, \n",
    "                  y_pred_gscv, \n",
    "                  rownames=['Actual Class'], \n",
    "                  colnames=['Predicted Class'] )\n",
    "\n",
    "print( f\"Confusion Matrix:\\n{ cm }\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :GradientBoostingClassifier\n",
    "# has no direct :class_weight\n",
    "# workaround: passing sample weights computed from class frequencies\n",
    "classes = np.unique( y_rs )\n",
    "\n",
    "class_w = compute_class_weight( 'balanced',\n",
    "                                classes=classes,\n",
    "                                y=y_rs )\n",
    "\n",
    "class_weight_dict = dict( zip(classes, class_w) )\n",
    "\n",
    "# per-sample weights from :class_weight_dict\n",
    "sample_weight = np.array( [class_weight_dict[c] for c in y_rs] )\n",
    "\n",
    "# init :GradientBoostingClassifier\n",
    "model_gbc = GradientBoostingClassifier( n_estimators=400,\n",
    "                                        learning_rate=0.05,\n",
    "                                        max_depth=3,\n",
    "                                        random_state=369 )\n",
    "\n",
    "# apply fit\n",
    "model_gbc.fit( X=X_rs,\n",
    "               y=y_rs,\n",
    "               sample_weight=sample_weight )\n",
    "\n",
    "# predict :X_test\n",
    "y_pred_gbc = model_gbc.predict( X_test )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.79      0.77     95219\n",
      "           2       0.73      0.49      0.59     93114\n",
      "           3       0.40      0.44      0.42     34672\n",
      "           4       0.14      0.62      0.23      5795\n",
      "\n",
      "    accuracy                           0.61    228800\n",
      "   macro avg       0.50      0.59      0.50    228800\n",
      "weighted avg       0.67      0.61      0.63    228800\n",
      "\n",
      "Confusion Matrix:\n",
      "Predicted Class      1      2      3      4\n",
      "Actual Class                               \n",
      "1                75279  10670   5066   4204\n",
      "2                22822  45916  16630   7746\n",
      "3                 3127   6247  15228  10070\n",
      "4                  324    474   1379   3618\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "print( classification_report(y_test, \n",
    "                             y_pred_gbc) )\n",
    "\n",
    "# confusion matrix\n",
    "cm = pd.crosstab( y_test,\n",
    "                  y_pred_gbc,\n",
    "                  rownames=['Actual Class'],\n",
    "                  colnames=['Predicted Class'] )\n",
    "\n",
    "print( f\"Confusion Matrix:\\n{ cm }\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a339dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :XGBoost with sample weights (multi-class)\n",
    "# multi-class imbalance -> using :sample_weight (one weight per instance)\n",
    "# expects lables to start at '0' -> :y_rs labels are 1-4\n",
    "\n",
    "# re-mapping labels\n",
    "# { 1, 2, 3, 4} -> { 0, 1, 2, 3 }\n",
    "y_rs_xgbc_encoded = y_rs - 1\n",
    "y_test_xgbc_encoded = y_test - 1\n",
    "\n",
    "# calc sample weights on encoded labels\n",
    "sample_weight = compute_sample_weight( 'balanced', \n",
    "                                       y_rs_xgbc_encoded )\n",
    "\n",
    "# init :XGBC\n",
    "model_xgbc = XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=4,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=369,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# apply fit on :X_rs, :y_rs_xgbc_encoded\n",
    "model_xgbc.fit( X_rs, \n",
    "                y_rs_xgbc_encoded, \n",
    "                sample_weight=sample_weight )\n",
    "\n",
    "# predict on :X_test\n",
    "# :XGBC will see classes [0,.., 3] as required\n",
    "y_pred_xgbc_encoded = model_xgbc.predict( X_test )\n",
    "\n",
    "# mapping back to original lables\n",
    "# evaluating everyting in original severity labels 1-4\n",
    "# 0..3 -> 1..4\n",
    "y_pred_xgbc = y_pred_xgbc_encoded + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6673cce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.79      0.77     95219\n",
      "           2       0.73      0.52      0.61     93114\n",
      "           3       0.41      0.47      0.44     34672\n",
      "           4       0.16      0.63      0.25      5795\n",
      "\n",
      "    accuracy                           0.63    228800\n",
      "   macro avg       0.51      0.60      0.52    228800\n",
      "weighted avg       0.68      0.63      0.64    228800\n",
      "\n",
      "Confusion Matrix:\n",
      "Predicted Class      1      2      3     4\n",
      "Actual Class                              \n",
      "1                75511  11129   5282  3297\n",
      "2                21326  48144  16958  6686\n",
      "3                 2845   6063  16424  9340\n",
      "4                  275    450   1403  3667\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "print( classification_report(y_test, \n",
    "                             y_pred_xgbc) )\n",
    "\n",
    "# confusion matrix\n",
    "cm = pd.crosstab( y_test,\n",
    "                  y_pred_xgbc,\n",
    "                  rownames=['Actual Class'],\n",
    "                  colnames=['Predicted Class'] )\n",
    "\n",
    "print( f\"Confusion Matrix:\\n{ cm }\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e6a1f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1997\n",
      "[LightGBM] [Info] Number of data points in the train set: 650000, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n"
     ]
    }
   ],
   "source": [
    "# :LightGBM with class weights\n",
    "# supports class weights natively\n",
    "classes = np.unique( y_rs )\n",
    "\n",
    "class_w = compute_class_weight( 'balanced',\n",
    "                                classes=classes, \n",
    "                                y=y_rs )\n",
    "\n",
    "class_weight = dict( zip(classes,\n",
    "                         class_w) )\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    objective=\"multiclass\",\n",
    "    num_class=4,\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=class_weight,\n",
    "    random_state=369,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# apply fit on :X_rs, :y_rs\n",
    "lgbm.fit( X_rs, \n",
    "          y_rs )\n",
    "\n",
    "# predict on :X_test\n",
    "y_pred_lgbm = lgbm.predict( X_test )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cd370eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.79      0.78     95219\n",
      "           2       0.73      0.53      0.62     93114\n",
      "           3       0.42      0.49      0.45     34672\n",
      "           4       0.17      0.62      0.27      5795\n",
      "\n",
      "    accuracy                           0.64    228800\n",
      "   macro avg       0.52      0.61      0.53    228800\n",
      "weighted avg       0.68      0.64      0.65    228800\n",
      "\n",
      "Confusion Matrix:\n",
      "Predicted Class      1      2      3     4\n",
      "Actual Class                              \n",
      "1                75107  11777   5420  2915\n",
      "2                20302  49765  16993  6054\n",
      "3                 2712   6202  17093  8665\n",
      "4                  253    454   1495  3593\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "print( classification_report(y_test, \n",
    "                             y_pred_lgbm) )\n",
    "\n",
    "# confusion matrix\n",
    "cm = pd.crosstab( y_test,\n",
    "                  y_pred_lgbm,\n",
    "                  rownames=['Actual Class'],\n",
    "                  colnames=['Predicted Class'] )\n",
    "\n",
    "print( f\"Confusion Matrix:\\n{ cm }\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e47dc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1997\n",
      "[LightGBM] [Info] Number of data points in the train set: 650000, number of used features: 40\n",
      "[LightGBM] [Info] Using self-defined objective function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds_project/lib/python3.13/site-packages/lightgbm/sklearn.py:1638: UserWarning: Cannot compute class probabilities or labels due to the usage of customized objective function.\n",
      "Returning raw scores instead.\n",
      "  _log_warning(\n"
     ]
    }
   ],
   "source": [
    "# :LightGBM with focal loss\n",
    "# stronger focus on minority samples\n",
    "# requires custom objective\n",
    "def focal_loss_lgb_multiclass(alpha=0.25, gamma=2.0):\n",
    "    \n",
    "    # returns a LightGBM-compatible loss function\n",
    "    def loss(y_true, preds):\n",
    "        # y_true: (n,), labels [ 1,..,4 ]\n",
    "        # preds: raw scores, shape (n * K,)\n",
    "        n_classes = 4\n",
    "        \n",
    "        # convert labels to int indices 0..K-1\n",
    "        # 1..4 -> 0..3\n",
    "        y_true = y_true.astype( int ) - 1\n",
    "        \n",
    "        # re-shape preds to (n, K)\n",
    "        preds = preds.reshape( n_classes, \n",
    "                               -1 ).T  \n",
    "        \n",
    "        # softmax to probabilities\n",
    "        exp_preds = np.exp( preds - preds.max(axis=1, \n",
    "                                              keepdims=True) )\n",
    "        \n",
    "        # (n, K)\n",
    "        prob = exp_preds / exp_preds.sum( axis=1, \n",
    "                                          keepdims=True )\n",
    "\n",
    "        # one-hot labels (n, K)\n",
    "        y_onehot = np.eye( n_classes )[y_true]\n",
    "        \n",
    "        # focal loss terms\n",
    "        # (n, )\n",
    "        pt = ( prob * y_onehot ).sum( axis=1 )\n",
    "        pt = np.clip( pt,\n",
    "                      1e-12,\n",
    "                      1.0 )\n",
    "        \n",
    "        grad_factor = alpha * (gamma * (1 - pt) ** (gamma - 1) * (-np.log(pt)) + (1 - pt) ** gamma) / ( pt )\n",
    "\n",
    "        # gradients\n",
    "        # (n, K)\n",
    "        grad = grad_factor[:, None] * ( prob - y_onehot )\n",
    "        \n",
    "        # hessians\n",
    "        # simple approx\n",
    "        hess = np.ones_like( grad )\n",
    "\n",
    "        return grad.T.reshape(-1), hess.T.reshape(-1)\n",
    "    return loss\n",
    "\n",
    "# init :LGBMClassifier\n",
    "lgbm_focal = lgb.LGBMClassifier(\n",
    "    num_class=4,\n",
    "    objective=focal_loss_lgb_multiclass(alpha=0.25, gamma=2.0),\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=369,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# apply fit on :X_rs, :y_rs\n",
    "# :y_rs -> re-mapping done by objective\n",
    "lgbm_focal.fit( X=X_rs, \n",
    "                y=y_rs )\n",
    "\n",
    "# predict on :X_test\n",
    "# get raw scores (n_samples, n_classes)\n",
    "# for multi-class this is already (n, K)\n",
    "proba = lgbm_focal.predict( X_test )\n",
    "\n",
    "# convert to predicted class indices 0..3\n",
    "y_pred_idx = proba.argmax( axis=1 )\n",
    "\n",
    "# mapping back\n",
    "# 0..3 -> 1..4\n",
    "y_pred_lgbm_focal = y_pred_idx + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7468a067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.20      0.15      0.17     95219\n",
      "           2       0.42      0.15      0.23     93114\n",
      "           3       0.37      0.29      0.33     34672\n",
      "           4       0.00      0.07      0.01      5795\n",
      "\n",
      "    accuracy                           0.17    228800\n",
      "   macro avg       0.25      0.17      0.18    228800\n",
      "weighted avg       0.31      0.17      0.21    228800\n",
      "\n",
      "Confusion Matrix:\n",
      "Predicted Class      1      2      3      4\n",
      "Actual Class                               \n",
      "1                14028   4935   5410  70846\n",
      "2                47180  14384   8487  23063\n",
      "3                 7753  13159  10181   3579\n",
      "4                  700   1457   3218    420\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "print( classification_report(y_test, \n",
    "                             y_pred_lgbm_focal) )\n",
    "\n",
    "# confusion matrix\n",
    "cm = pd.crosstab( y_test,\n",
    "                  y_pred_lgbm_focal,\n",
    "                  rownames=['Actual Class'],\n",
    "                  colnames=['Predicted Class'] )\n",
    "\n",
    "print( f\"Confusion Matrix:\\n{ cm }\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# save to google cloud bucket\n",
    "# ---------------------------\n",
    "gc_storage.upload( bucket=bucket,\n",
    "                   obj=df,\n",
    "                   local_folder='2_preprocessing',\n",
    "                   file_name='1.0-munz-data-preprocessing_resampling.joblib' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
